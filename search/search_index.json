{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Beast API Documentation Welcome to the documentation of the BEAST API Project! Getting Started to Develop! The project requires JDK v1.8 and Spark v3.0.1 for the BEAST library, ensure SPARK_HOME and JAVA_HOME are in path. Pull the repository using git clone git@github.com:abraham2512/beasttools-maven.git Maven is used to compile dependencies and launch the project Compile the project using IntelliJ with compile-time dependencies and run with StartApp as main class Project layout src/ main/ scala/ StartApp.scala # Main function and entry point to the server. actors # Contains all the actors Routes.scala # ScalaDSL routing logic. FileRegistry.scala # Helper object for the HTTP routing Actor HdfsActor.scala # Actor for indexing and partitioning of data using BEAST TileActor.scala # Actor for rendering tiles on the fly using index models DataFileDAL.scala # Relational Mapping for H2 metadata database DataFileDAO.scala utils JsonFormats.scala resources/ application.conf #Configuration File for the server pom.xml # Maven dependencies Building Maven-shade-plugin is used to build into a Uber JAR using mvn clean package Deploying The Uber Jar with all dependencies can be deployed on Spark with spark-submit Build dependenices not clean/working at the time of writing . Additional Documentation Beast Documentation AkkaHTTP Documentation Akka Concurrency by Derek Wyatt - A great book that runs through the basics of Akka!","title":"Home"},{"location":"#welcome-to-beast-api-documentation","text":"Welcome to the documentation of the BEAST API Project!","title":"Welcome to Beast API Documentation"},{"location":"#getting-started-to-develop","text":"The project requires JDK v1.8 and Spark v3.0.1 for the BEAST library, ensure SPARK_HOME and JAVA_HOME are in path. Pull the repository using git clone git@github.com:abraham2512/beasttools-maven.git Maven is used to compile dependencies and launch the project Compile the project using IntelliJ with compile-time dependencies and run with StartApp as main class","title":"Getting Started to Develop!"},{"location":"#project-layout","text":"src/ main/ scala/ StartApp.scala # Main function and entry point to the server. actors # Contains all the actors Routes.scala # ScalaDSL routing logic. FileRegistry.scala # Helper object for the HTTP routing Actor HdfsActor.scala # Actor for indexing and partitioning of data using BEAST TileActor.scala # Actor for rendering tiles on the fly using index models DataFileDAL.scala # Relational Mapping for H2 metadata database DataFileDAO.scala utils JsonFormats.scala resources/ application.conf #Configuration File for the server pom.xml # Maven dependencies","title":"Project layout"},{"location":"#building","text":"Maven-shade-plugin is used to build into a Uber JAR using mvn clean package","title":"Building"},{"location":"#deploying","text":"The Uber Jar with all dependencies can be deployed on Spark with spark-submit Build dependenices not clean/working at the time of writing .","title":"Deploying"},{"location":"#additional-documentation","text":"Beast Documentation AkkaHTTP Documentation Akka Concurrency by Derek Wyatt - A great book that runs through the basics of Akka!","title":"Additional Documentation"},{"location":"backend/","text":"Akka HTTP Endpoints POST /files Starts the download for a file if it doesnt exist. curl -X POST http://127.0.0.1:8080/files -d '{\"filename\": \"SafetyDept\",\"filetype\": \"shapefile\",\"filesource\": \"/some/path/to/file\",\"filestatus\": \"start\"}' Sample Response Status 201 - Created { \"description\": \"created\" } GET /files Return all the files in our server curl -X GET http://127.0.0.1:8080/files Sample Response Status 200 - OK { \"files\": [ { \"filename\": \"SafetyDept\", \"filesource\": \"/some/path/to/file\", \"filestatus\": \"indexed\", \"filetype\": \"default\" }, { \"filename\": \"Sections\", \"filesource\": \"/some/path/to/file\", \"filestatus\": \"indexed\", \"filetype\": \"shapefile\" } ] } GET /files/{id} Returns the details of a file curl -X GET http://127.0.0.1:8080/files/SafetyDept Sample Response Status 200 - OK { \"filename\": \"SafetyDept\", \"filesource\": \"/some/path/to/file\", \"filestatus\": \"indexed\", \"filetype\": \"shapefile\" } DELETE /files/{id} Deletes the dataset from server curl -X DELETE http://127.0.0.1:8080/files/SafetyDept Sample Response Status 200 - OK { \"description\": \"deleted\" } GET /tiles/ Returns pre generated tile or generates one on the fly. curl -X GET http://127.0.0.1:8080/tiles?dataset=<D>&z=<Z>&x=<X>&y=<Y> Returns a tile image for the dataset D with coordinates Z, X, Y","title":"Backend"},{"location":"backend/#akka-http-endpoints","text":"","title":"Akka HTTP Endpoints"},{"location":"backend/#post-files","text":"Starts the download for a file if it doesnt exist. curl -X POST http://127.0.0.1:8080/files -d '{\"filename\": \"SafetyDept\",\"filetype\": \"shapefile\",\"filesource\": \"/some/path/to/file\",\"filestatus\": \"start\"}' Sample Response Status 201 - Created { \"description\": \"created\" }","title":"POST /files"},{"location":"backend/#get-files","text":"Return all the files in our server curl -X GET http://127.0.0.1:8080/files Sample Response Status 200 - OK { \"files\": [ { \"filename\": \"SafetyDept\", \"filesource\": \"/some/path/to/file\", \"filestatus\": \"indexed\", \"filetype\": \"default\" }, { \"filename\": \"Sections\", \"filesource\": \"/some/path/to/file\", \"filestatus\": \"indexed\", \"filetype\": \"shapefile\" } ] }","title":"GET /files"},{"location":"backend/#get-filesid","text":"Returns the details of a file curl -X GET http://127.0.0.1:8080/files/SafetyDept Sample Response Status 200 - OK { \"filename\": \"SafetyDept\", \"filesource\": \"/some/path/to/file\", \"filestatus\": \"indexed\", \"filetype\": \"shapefile\" }","title":"GET /files/{id}"},{"location":"backend/#delete-filesid","text":"Deletes the dataset from server curl -X DELETE http://127.0.0.1:8080/files/SafetyDept Sample Response Status 200 - OK { \"description\": \"deleted\" }","title":"DELETE /files/{id}"},{"location":"backend/#get-tiles","text":"Returns pre generated tile or generates one on the fly. curl -X GET http://127.0.0.1:8080/tiles?dataset=<D>&z=<Z>&x=<X>&y=<Y> Returns a tile image for the dataset D with coordinates Z, X, Y","title":"GET /tiles/"},{"location":"frontend/","text":"Maps Interface The interface is built with OpenLayers and JavaScript. Start your own front-end OpenLayers.js can be imported as a browser script but modern JavaScript works best when using and authoring modules. The recommended way of using OpenLayers is installing the ol package. This can be done through node using the command npx create-ol-app . For more information, see the guide here Clone Beast Viz To clone the default front-end, run this command to clone the repository git clone git@github.com:abraham2512/beasttools-maven.git and open the frontend/beastol/ project folder. The project has index.js , index.html , styles.css and a package.json configuration file. Setup the interface First install node modules from package.json with npm install . For development run the server using npm run start . For deployment bundle the files using npm run build to create one index.html file that can be served. index.js function launchMap(filename) :Creates a map with the filename ID. :Makes GET request to /tiles/ endpoint in backend with dataset = {filename} and {Z}, {X}, {Y} values. function handleDataFileSubmit(event) :Handles form submit with data json. :Makes POST request to /files/ endpoint with data payload. This triggers partitioning and indexing of data. function appendCardDiv(filename) :Appends a new Card for the dataset in the UI to track its progress and launch when ready. index.html This file imports the index.js module and contains the main html.","title":"Frontend"},{"location":"frontend/#maps-interface","text":"The interface is built with OpenLayers and JavaScript.","title":"Maps Interface"},{"location":"frontend/#start-your-own-front-end","text":"OpenLayers.js can be imported as a browser script but modern JavaScript works best when using and authoring modules. The recommended way of using OpenLayers is installing the ol package. This can be done through node using the command npx create-ol-app . For more information, see the guide here","title":"Start your own front-end"},{"location":"frontend/#clone-beast-viz","text":"To clone the default front-end, run this command to clone the repository git clone git@github.com:abraham2512/beasttools-maven.git and open the frontend/beastol/ project folder. The project has index.js , index.html , styles.css and a package.json configuration file.","title":"Clone Beast Viz"},{"location":"frontend/#setup-the-interface","text":"First install node modules from package.json with npm install . For development run the server using npm run start . For deployment bundle the files using npm run build to create one index.html file that can be served.","title":"Setup the interface"},{"location":"frontend/#indexjs","text":"","title":"index.js"},{"location":"frontend/#function-launchmapfilename","text":":Creates a map with the filename ID. :Makes GET request to /tiles/ endpoint in backend with dataset = {filename} and {Z}, {X}, {Y} values.","title":"function launchMap(filename)"},{"location":"frontend/#function-handledatafilesubmitevent","text":":Handles form submit with data json. :Makes POST request to /files/ endpoint with data payload. This triggers partitioning and indexing of data.","title":"function handleDataFileSubmit(event)"},{"location":"frontend/#function-appendcarddivfilename","text":":Appends a new Card for the dataset in the UI to track its progress and launch when ready.","title":"function appendCardDiv(filename)"},{"location":"frontend/#indexhtml","text":"This file imports the index.js module and contains the main html.","title":"index.html"},{"location":"overview/","text":"Beast Microservice The microservice backend is to enable upload and processing of Spatial-Temporal data files on the Hadoop using Spark and BEAST library. There were a few design considerations and AkkaHTTP was deemed a good fit for the API. It is a lightweight but feature-rich and highly scalable toolkit for building API endpoints based on the Actor Model and message passing. Akka and AkkaHTTP Concurrency through message passing (Actors) Non-blocking by default (Futures) Fault Tolerance (a.k.a. Resiliency, \u2018let it crash\u2019 model) By building the endpoints using actors and separating the indexing/partitioning jobs and tile generation jobs, we can achieve a degree of parallelism in our API. Our project has three Actors Actors in BeastAPI Routes.scala : Entry point and handles routing logic. Passes messages to TileActor and HdfsActor HdfsActor.scala : Partitioning and indexing job messages are received and performed by this Actor TileActor.scala : Tiles are generated on the fly from indexes by this actor The actors can be deployed and scaled horizontally into the cluster using the Akka Cluster API. The following video is a good source of motivation to use Akka-Cluster Current Architecture design Using Akka ActorContext and Behaviors A Scala object can be made into an Akka actor simply by importing akka.actor.typed.scaladsl.ActorContext and akka.actor.typed.scaladsl.Behaviors into your scala object. Use Behaviours.setup to create a match case for ActorContext . Any message passed to this actor will be processed depending on its match case. A simple example would be object MyActor{ def apply(): Behavior[CommandTrait] = Behaviors.setup { context: ActorContext[CommandTrait] => println(\"actors.HdfsRegistry: Hdfs Actor awake\") Behaviors.receiveMessage { case SpeakText(msg) => println(s\"actors.HdfsActor: got a msg: $msg\") Behaviors.same case _ => println(\"Default Case\") Behaviors.same } } } For the Actor to be visible to other actors in the server, we can register it to the Receptionist . Something like this would tell the receptionist to register the actor. context.system.receptionist ! Receptionist.Register(\"KeyForNewActor\", context.self)","title":"Overview"},{"location":"overview/#beast-microservice","text":"The microservice backend is to enable upload and processing of Spatial-Temporal data files on the Hadoop using Spark and BEAST library. There were a few design considerations and AkkaHTTP was deemed a good fit for the API. It is a lightweight but feature-rich and highly scalable toolkit for building API endpoints based on the Actor Model and message passing.","title":"Beast Microservice"},{"location":"overview/#akka-and-akkahttp","text":"Concurrency through message passing (Actors) Non-blocking by default (Futures) Fault Tolerance (a.k.a. Resiliency, \u2018let it crash\u2019 model) By building the endpoints using actors and separating the indexing/partitioning jobs and tile generation jobs, we can achieve a degree of parallelism in our API. Our project has three Actors","title":"Akka and AkkaHTTP"},{"location":"overview/#actors-in-beastapi","text":"Routes.scala : Entry point and handles routing logic. Passes messages to TileActor and HdfsActor HdfsActor.scala : Partitioning and indexing job messages are received and performed by this Actor TileActor.scala : Tiles are generated on the fly from indexes by this actor The actors can be deployed and scaled horizontally into the cluster using the Akka Cluster API. The following video is a good source of motivation to use Akka-Cluster","title":"Actors in BeastAPI"},{"location":"overview/#current-architecture-design","text":"","title":"Current Architecture design"},{"location":"overview/#using-akka-actorcontext-and-behaviors","text":"A Scala object can be made into an Akka actor simply by importing akka.actor.typed.scaladsl.ActorContext and akka.actor.typed.scaladsl.Behaviors into your scala object. Use Behaviours.setup to create a match case for ActorContext . Any message passed to this actor will be processed depending on its match case. A simple example would be object MyActor{ def apply(): Behavior[CommandTrait] = Behaviors.setup { context: ActorContext[CommandTrait] => println(\"actors.HdfsRegistry: Hdfs Actor awake\") Behaviors.receiveMessage { case SpeakText(msg) => println(s\"actors.HdfsActor: got a msg: $msg\") Behaviors.same case _ => println(\"Default Case\") Behaviors.same } } } For the Actor to be visible to other actors in the server, we can register it to the Receptionist . Something like this would tell the receptionist to register the actor. context.system.receptionist ! Receptionist.Register(\"KeyForNewActor\", context.self)","title":"Using Akka ActorContext and Behaviors"}]}