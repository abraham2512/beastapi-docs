{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Beast API Documentation Welcome to the documentation of the BEAST API project! Getting Started mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout src/ main/ scala/ StartApp.scala # Main function and entry point to the server. actors # Contains all the actors Routes.scala # ScalaDSL routing logic. FileRegistry.scala # Helper object for the HTTP routing Actor HdfsActor.scala # Actor for indexing and partitioning of data using BEAST TileActor.scala # Actor for rendering tiles on the fly using index models DataFileDAL.scala # Relational Mapping for H2 metadata database DataFileDAO.scala Utils JsonFormats.scala resources/ application.conf #Configuration File for the server pom.xml # Maven dependencies Additional Documentation","title":"Home"},{"location":"#welcome-to-beast-api-documentation","text":"Welcome to the documentation of the BEAST API project!","title":"Welcome to Beast API Documentation"},{"location":"#getting-started","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Getting Started"},{"location":"#project-layout","text":"src/ main/ scala/ StartApp.scala # Main function and entry point to the server. actors # Contains all the actors Routes.scala # ScalaDSL routing logic. FileRegistry.scala # Helper object for the HTTP routing Actor HdfsActor.scala # Actor for indexing and partitioning of data using BEAST TileActor.scala # Actor for rendering tiles on the fly using index models DataFileDAL.scala # Relational Mapping for H2 metadata database DataFileDAO.scala Utils JsonFormats.scala resources/ application.conf #Configuration File for the server pom.xml # Maven dependencies","title":"Project layout"},{"location":"#additional-documentation","text":"","title":"Additional Documentation"},{"location":"backend/","text":"Akka HTTP Endpoints GET /tiles/ Returns pre generated tile or generates one on the fly. curl http://127.0.0.1:8080/tiles?dataset=<dataset>&z=<Z>&x=<X>&y=<Y> GET /files Return all the files in our server curl http://127.0.0.1:8080/files POST /files Upload the file from URL into our HDFS and update metadata in embedded SQL storage GET /files/{id} -> Returns the details of a file DELETE /files/{id} Deletes the dataset from server","title":"Backend"},{"location":"backend/#akka-http-endpoints","text":"","title":"Akka HTTP Endpoints"},{"location":"backend/#get-tiles","text":"Returns pre generated tile or generates one on the fly. curl http://127.0.0.1:8080/tiles?dataset=<dataset>&z=<Z>&x=<X>&y=<Y>","title":"GET /tiles/"},{"location":"backend/#get-files","text":"Return all the files in our server curl http://127.0.0.1:8080/files","title":"GET /files"},{"location":"backend/#post-files","text":"Upload the file from URL into our HDFS and update metadata in embedded SQL storage","title":"POST /files"},{"location":"backend/#get-filesid","text":"-> Returns the details of a file","title":"GET /files/{id}"},{"location":"backend/#delete-filesid","text":"Deletes the dataset from server","title":"DELETE /files/{id}"},{"location":"frontend/","text":"","title":"Frontend"},{"location":"overview/","text":"Beast Microservice The microservice backend is to enable upload and processing of Spatial-Temporal data files on the Hadoop using Spark and BEAST library. There were a few design considerations and AkkaHTTP was deemed a good fit for the API. It is a lightweight but feature-rich and highly scalable toolkit for building API endpoints based on the Actor Model. AkkaHTTP was chosen Concurrency through message passing (Actors) Non-blocking by default (Futures) Fault Tolerance (a.k.a. Resiliency, \u2018let it crash\u2019 model)","title":"Overview"},{"location":"overview/#beast-microservice","text":"The microservice backend is to enable upload and processing of Spatial-Temporal data files on the Hadoop using Spark and BEAST library. There were a few design considerations and AkkaHTTP was deemed a good fit for the API. It is a lightweight but feature-rich and highly scalable toolkit for building API endpoints based on the Actor Model. AkkaHTTP was chosen Concurrency through message passing (Actors) Non-blocking by default (Futures) Fault Tolerance (a.k.a. Resiliency, \u2018let it crash\u2019 model)","title":"Beast Microservice"}]}